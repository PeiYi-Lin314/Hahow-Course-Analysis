# 用數據找先機 | 線上課程平台的潛力領域探索   

## 📌 專案介紹

本專案透過 `Python` 自動化爬取 Hahow 平台課程資訊，並完成資料清理與整合，產出結構化且可用於分析的 `CSV` 檔案。

後續將利用 `Power BI` 及 `文字雲關鍵字分析`，探索潛力課程，找出市場中具有高潛力但尚未飽和的領域，作為課程開發與行銷策略的參考依據。 

目前程式仍在持續修正中，但已完成初步爬取與清理階段，並將文字雲與關鍵字分析結果展示於 Notion 作品集。
  
👉 [Hahow 官網](https://hahow.in/)    

👉 [用數據找先機 | 本專案 Notion 連結](https://gold-twist-553.notion.site/1e047c95fe3580d88b1dff7b681ef583)  

⚠️ 本專案資料皆取自 Hahow 官網公開頁面，**僅供學術與個人學習用途，無商業意圖**。如有侵權疑慮，歡迎聯絡我以便下架處理。 

---
## 📌 專案結構
<pre>
hahow_course_analysis/  
├── README.md          # 專案說明文件  
├── scraped_data/      # 每頁課程的原始爬蟲資料（程式執行後自動產生）         
├── merged_data/       # 每個分類合併後的原始資料（程式執行後自動產生）                            
├── cleaned_data/      # 清理後的分類資料（程式執行後自動產生）  
├── total_data/        # 最終彙整的課程總表（程式執行後產出）
├── 01_course_scraping.ipynb    # 爬蟲主程式（爬取每一分類）
├── 02_data_merging.ipynb       # 合併各分類 CSV 檔案  
└── 03_data_cleaning.ipynb      # 資料清理與整理合併檔案
</pre>
---
## 📌 環境設定  

### 🔧 必備軟體與套件版本
- Python 3.12.7
- Selenium 4.29.0（搭配 Chrome WebDriver）
- BeautifulSoup 4.12.3
- pandas 2.2.2
---

### 🌐 ChromeDriver 下載與設定    

本專案使用 Selenium 進行網頁自動化爬取，需搭配對應版本的 ChromeDriver。  

✅ ChromeDriver 安裝說明  
1. 請確認你已安裝 Google Chrome。
2. 根據你的本機安裝的作業系統與 Chrome 版本至以下頁面下載對應版本的 ChromeDriver。
3. 解壓縮後，取得 `chromedriver.exe` 檔案。
            
👉 [ChromeDriver 官方下載頁面](https://developer.chrome.com/docs/chromedriver/downloads?hl=zh-tw)

📁 建議放置位置
將 `chromedriver.exe` 放置在專案的根目錄中，例如：
<pre>
hahow_course_analysis/  
├── chromedriver.exe            ← ✅ 放這裡                                 
├── 01_course_scraping.ipynb    # 爬蟲主程式（爬取每一分類）  
├── ...     
</pre>

程式已設定自動尋找專案資料夾內的 `chromedriver.exe`，不需額外加入系統環境變數：
```python
from selenium.webdriver.chrome.service import Service
service = Service(executable_path="./chromedriver.exe")
driver = webdriver.Chrome(service=service, options=chrome_options)
```

---
## 📌 程式特色  
- **Selenium + BeautifulSoup 自動化網頁爬取**
  
    - 結合 `Selenium`的瀏覽器模擬與 `BeautifulSoup` 的靜態網頁解析，提升資料抓取的穩定性與效率。搭配隨機 `User-Agent` 與 `Proxy` 設定，可降低被反爬蟲封鎖的風險，且易於擴充。
    
- **靈活的正則表達式解析**
  
    - 針對未結構化且格式多變的課程資訊，設計專用正則表達式自動擷取重要欄位，大幅減少人工清理時間，並提升資料完整性。

- **嚴謹的錯誤處理與資料清理流程**
  
    - 包含缺失值檢查、格式轉換與數值標準化，確保輸出資料乾淨且一致，避免分析時因資料異常造成誤判。
      
- **高度擴充性設計**
  
    - 僅需新增 `course_type` 對應字典，即可快速加入更多課程分類，支援後續資料規模擴張與不同分類的靈活應用。
      
- **自動過濾異常資料**
  
    - 清理過程自動剔除缺少關鍵欄位或格式不符的資料列，保持資料品質穩定，為後續分析提供可靠基礎。
      
- **統一格式，方便分析工具直接使用**
  
    - 合併後資料欄位名稱與格式一致，方便無縫接軌 `Power BI`等商業分析工具，快速建立互動式報表與視覺化分析。
---
## 📌 分析流程

## 📍 PART 1｜Hahow 課程資料擷取

本專案第一階段使用 `Selenium` 搭配 `BeautifulSoup`，自動擷取 Hahow 各分類課程資料。

- 每個分類的課程頁面會逐頁抓取課程資訊，包含課程名稱、課程連結、熱門標籤等欄位。
- 擷取後的資料會自動儲存為 CSV 檔，存放於 `scraped_data/` 資料夾中。
- CSV 檔案依據課程分類與頁碼命名，例如 `hahow_音樂_page1.csv`，方便後續資料整理與管理。

此流程確保資料擷取自動化且系統化，為後續資料清理與分析打下穩固基礎。

### 🛠️ 執行說明

1. 安裝必要套件（請參見上方 **環境設定** 章節，手動安裝所需套件）。
  
2. 執行主程式 `01_course_scraping.ipynb`。
  
3. 可於程式中依需求調整以下變數：
   - `course_type`：設定欲爬取的課程分類。
   - `user_agents_list` / `proxies_list`：自訂並隨機使用 User-Agent 和 Proxy，模擬正常使用者行為以避免被封鎖。

### ⚠️ 爬蟲注意事項

1. **網站資料為 JavaScript 動態渲染**，課程區塊的 class（如 `gtWJhG`）可能會變動。  
   - 若無法擷取課程資料，請開啟開發者工具（F12）檢查 HTML 結構，並更新 `01_course_scraping.ipynb` 中對應的 class 名稱。

2. **若出現「網頁無法開啟」或「爬取速度變慢」的情況：**  
   - 可能是 Proxy 過期，請進行更新。  
   - Hahow 可能調整了網站結構，請檢查並修正爬蟲邏輯。

### ✅ 實際執行紀錄與錯誤處理展示   

以下為實際執行 `01_course_scraping.ipynb` 程式時的部分輸出紀錄，展示爬蟲具備錯誤重試機制，能穩定處理偶發性錯誤並完整擷取資料，執行過程如下：
```diff
- 攝影第3頁發生錯誤並重試1次: Message:
第3頁爬取成功，並輸出csv完成
第4頁爬取成功，並輸出csv完成
攝影 已爬取完畢，共爬取到第 4 頁，進行下一個分類
全部爬取完成,並關閉瀏覽器
耗時358.2056610584259
```
上述紀錄說明本程式能穩定處理多分類與多頁數資料擷取，透過重試機制成功解決突發錯誤，確保資料完整性。  
**此輸出為程式執行時的真實紀錄，有助於驗證程式執行狀態。**

---
## 📍 PART 2｜各分類資料合併

程式 `02_data_merging.ipynb` 會自動讀取 `scraped_data/` 資料夾中、依課程分類儲存的所有分頁 CSV 檔，並合併為單一資料集，並輸出至 `merged_data/` 資料夾中。

例如，若「音樂」類別共有兩頁，則會讀取： 
- `hahow_音樂_page1.csv`  
- `hahow_音樂_page2.csv`  

合併後輸出為：  
- `hahow_音樂_merged.csv`  

此步驟可有效彙整跨頁資料，方便後續資料清理與分析使用。

### 🧩 合併注意事項

- 程式會自動為每筆資料加入「分類」欄位，便於後續分析。
- 檔案命名格式為 `hahow_{分類名稱}_merged.csv`，可支援自動化讀取與錯誤追蹤。
- 若某分類資料缺少頁數，合併程序會提早結束，便於根據檔名快速定位中斷或遺漏頁數的問題。
- 使用 `pandas.concat()` 進行合併，並設定 `ignore_index=True`，確保合併後的資料索引一致且整齊。

### ✅ 實際執行紀錄展示
以下為實際執行 `02_data_merging.ipynb` 程式時的部分輸出紀錄，展示程式能夠依序讀取各分類各頁的 CSV 檔案，並在找不到下一頁資料時自動跳過，完成各分類資料合併並輸出至指定資料夾 `merged_data/` 中，執行過程如下：
```
已建立 merged_data 資料夾
完成讀取: scraped_data/hahow_攝影_page1.csv  
完成讀取: scraped_data/hahow_攝影_page2.csv  
完成讀取: scraped_data/hahow_攝影_page3.csv  
完成讀取: scraped_data/hahow_攝影_page4.csv  
攝影第5頁資料不存在，進行下一個分類...  
輸出 hahow攝影合併.csv 完成  

全部分類皆合併完成!
```
上述紀錄說明本程式具備自動偵測資料存在與否的能力，確保資料完整合併，方便後續資料分析使用。  
**此輸出為程式執行時的真實紀錄，有助於驗證程式執行狀態。**

---
## 📍 PART 3｜清理資料

程式 `03_data_cleaning.ipynb` 會依序讀取 `merged_data/` 資料夾中各分類的 CSV 檔案（檔名格式為 `hahow_{分類名稱}_merged.csv`），並進行資料清理：

- 詳細正則表達式與拆解邏輯請參考程式碼註解。
- 「課程名稱」格式多變，正則表達式已涵蓋常見變化（如「限時優惠」、「by」、「$NT」前後空白等），以提高擷取準確度。
- 處理文字中的數字格式（含千分位符號、時間單位換算），並轉換為數值型態。
- 若某筆資料缺少關鍵欄位（如評分），該筆資料將被刪除，確保後續資料分析的完整性與準確性。
- 欄位名稱與順序已統一設計，方便後續直接匯入 Power BI 等分析工具。

清理完成後，各分類的資料會儲存於 `cleaned_data/` 資料夾中，檔名格式為 `hahow_{分類名稱}_cleaned.csv`。

### 🔍 最終清理後的輸出欄位

| 欄位名稱 | 說明                             |
|----------|----------------------------------|
| 分類     | 課程所屬分類（例如：音樂、語言等） |
| 熱門標籤 | 原始爬蟲資料中提供的課程標籤（熱門課程／無標籤） |
| 課程     | 課程名稱                         |
| 講師     | 課程講師                         |
| 評分     | 課程評分（例如：4.8）            |
| 評價數   | 評分人數                         |
| 課程時長 | 課程時長（單位：小時）           |
| 購買人數 | 購買人數                         |
| 價錢     | 課程價格                         |
| link     | 完整課程網址                     |


最後，程式會自動合併所有清理後的分類資料，生成完整的總表，作為後續分析依據。

📌 合併總表說明：

- 檔名為：`hahow_all_courses.csv`
- 儲存資料夾：`total_data/`

### ✅ 實際執行紀錄展示  
`03_data_cleaning.ipynb` 會針對爬取的原始 CSV 資料進行清理，並輸出至 `cleaned_data/` 資料夾中，執行過程如下：
```
已建立 cleaned_data 資料夾
音樂 清理完成  
hahow_音樂_cleaned.csv 輸出完成  
攝影 清理完成  
hahow_攝影_cleaned.csv 輸出完成  

全部分類完成!!
```
當各分類資料清理完成後，`03_data_cleaning.ipynb` 中的合併程式區塊會將清理後的資料合併成一份完整課程資料，並輸出至 `total_data/` 資料夾：
```
已建立 total_data 資料夾  
hahow_音樂_cleaned.csv 讀取成功  
hahow_攝影_cleaned.csv 讀取成功  
hahow_all_courses.csv 輸出完成
```
合併結果方便後續進行資料分析與視覺化處理。  
**以上兩段輸出皆為程式執行時的真實紀錄，有助於驗證程式執行狀態。**

---
## 🛠 未來可擴充方向
- 加入定期排程機制，自動更新課程資訊。
- 擴充資料欄位（如：上架日期、開課狀態、募資狀態、導師回應數等），提升資料探討深度。
- 整合 Power BI 等商業分析工具進行視覺化（目前已有 Power BI 相關作法，文字雲及關鍵字分析部分仍待測試 Jupyter Notebook 執行狀況）。

## ⚠️ 已知問題與改進方向
- 部分資料欄位（講師、評分）拆分仍不夠精確，因原始資料中講師名稱可能包含數字，且與評分數值連續出現，導致解析時無法正確分隔，產生類似「小明老師1234565.0」的錯誤結果。
- 資料格式異常時的錯誤處理機制有待強化，提升程式穩定性。
- 新增爬蟲失敗頁面的紀錄與補爬機制
- 增加 log 檔以儲存完整爬取過程，便於後續追蹤與除錯。
- 目前僅在 Jupyter Notebook 針對部分分類測試程式運作，後續需完成所有領域的資料爬取與整合。
---

## 🙋‍♂️ 聯絡方式

感謝閱讀本專案！
  
若您有任何建議、想交流的地方，或對其他作品感興趣，歡迎聯絡我：  

📧 Email：cing880314@gmail.com  
  
📂 Notion：[ 完整作品集整理 ](https://gold-twist-553.notion.site/1e047c95fe3580d3b28cd0573424d909)

---

## 📌 版本紀錄

- **v1.0 （2025-05-15）**

  - 完成基本爬蟲功能：資料爬取、合併與清理、匯出 CSV。  
  - 建立專案架構與流程說明。  
  - 撰寫環境設定與程式特色。

- **v1.1 （2025-05-18）**

  - 新增重試機制，提升爬取穩定性。。  
  - 修正部分欄位拆分問題（如講師與評分欄位）。  
  - 增加爬蟲效能測試與時間紀錄。
  - 發現並記錄少數資料格式異常，留待後續修正程式拆分邏輯。

