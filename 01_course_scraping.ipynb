{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ad8600",
   "metadata": {},
   "source": [
    "\n",
    "# Hahow 課程資料爬蟲分析\n",
    "透過 Selenium 與 BeautifulSoup 爬取 Hahow 平台上各類別的課程資訊，輸出為 CSV 檔供後續資料分析使用。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec9e6d",
   "metadata": {},
   "source": [
    "\n",
    "### **📌專案目的**\n",
    "\n",
    "模擬使用者在網站上的瀏覽與選課行為，抓取每一類別下的所有課程標題、熱門標籤與連結。\n",
    "\n",
    "### **📋 使用工具**\n",
    "\n",
    "- Python\n",
    "- Selenium\n",
    "- BeautifulSoup\n",
    "- Pandas\n",
    "- ChromeDriver\n",
    "\n",
    "### **🔧 爬蟲流程介紹**\n",
    "\n",
    "\n",
    "#### **1. 建立課程分類與對應網址代碼字典**\n",
    "   - 將 Hahow 平台的課程分類轉換為可用於爬蟲的網址參數\n",
    "#### **2. 設定 User-agent 與 Proxy**\n",
    "   - 模擬一般使用者行為，降低被封鎖的風險\n",
    "#### **3. 使用 Selenium 開啟課程分類頁面**\n",
    "   - 自動化瀏覽器操作以取得完整頁面內容\n",
    "#### **4. 使用 BeautifulSoup 解析課程清單區塊**\n",
    "   - 提取 HTML 中每門課程的基本資訊\n",
    "#### **5. 撰寫自定義函數處理單一課程資料**\n",
    "   - 擷取課程名稱、熱門標籤、連結共三個欄位\n",
    "#### **6. 將結構化資料存為 CSV 檔案**\n",
    "   - 每分類每頁資料儲存於 `scraped_data/`資料夾，便於後續分析\n",
    "#### **7. 加入例外處理與計時功能**\n",
    "   - 確保流程穩定，並評估整體執行效能\n",
    "\n",
    "### **⚠️ 爬蟲注意事項**\n",
    "\n",
    "- **本程式中的代理 IP 與 User-Agent 僅為示範用途**，實際使用時請更換為有效的 Proxy 與 User-Agent，請注意免費代理有可能失效，請定期更新以避免請求失敗或遭平台封鎖。\n",
    "- **Hahow 頁面為動態生成，課程區塊的 `class` 名稱（如 `gtWJhG`）可能會變動**，若無法擷取資料請參考原始碼更新 class。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fbaa73-71ee-4aa8-bd44-8e735d463ebc",
   "metadata": {},
   "source": [
    "#### 模組匯入區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295d393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 匯入基本模組與爬蟲相關工具\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service #路徑\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcc2ade-b2dd-46a9-8d8e-a7916dbf0145",
   "metadata": {},
   "source": [
    "#### 自訂函數區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7008e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定義函數：處理每一筆課程資料，擷取課程名稱、熱門標籤與連結\n",
    "\n",
    "def course_item(data):\n",
    "    course_list = [] # 用來儲存處理後的課程資料\n",
    "\n",
    "    for item in data:\n",
    "        \n",
    "        # 擷取課程名稱並去除多餘空白\n",
    "        text = item.text.strip() \n",
    "\n",
    "        # 尋找課程的連結標籤，若有則提取連結，若無則顯示為「無連結」\n",
    "        link_tag = item.find(\"a\") \n",
    "        link = link_tag.get(\"href\") if link_tag else \"無連結\"  \n",
    "\n",
    "        # 尋找熱門標籤所在的 div，若有則提取文字，否則顯示為「無標籤」\n",
    "        popular_tag = item.find(\"div\",\"sc-1x67v6w-0 sc-1x67v6w-1 jYbAhu icUvip\")\n",
    "        popular = popular_tag.text.strip() if popular_tag else \"無標籤\"\n",
    "\n",
    "        # 將每筆課程資料加入清單\n",
    "        course_list.append([text, popular, link])\n",
    "\n",
    "    return course_list # 回傳所有課程資料\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3bedf-c6c2-4fb5-be07-d0c492106f4a",
   "metadata": {},
   "source": [
    "#### 程式設定區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "299c132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義課程類別與對應網址代碼\n",
    "\n",
    "course_type = {\n",
    "    \"音樂\": \"music\", \"語言\": \"language\", \"攝影\": \"photography\", \"藝術\": \"art\",\n",
    "    \"設計\": \"design\", \"人文\": \"humanities\", \"行銷\": \"marketing\",\n",
    "    \"程式\": \"programming\", \"投資理財\": \"finance-and-investment\",\n",
    "    \"職場技能\": \"career-skills\", \"手作\": \"handmade\", \"生活品味\": \"lifestyle\"\n",
    "}\n",
    "\n",
    "# 設定 User-Agent 清單 \"音樂\": \"music\",\"人文\":\"humanities\",\n",
    "user_agents_list = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:110.0) Gecko/20100101 Firefox/110.0\"\n",
    "]\n",
    "\n",
    "# 設定 Proxies 清單 \n",
    "proxies_list = [\n",
    "    \"41.223.119.156:3128\"\n",
    "]\n",
    "\n",
    "# 隨機選取一個 User-Agent 和 proxy，降低被網站偵測為機器人的風險\n",
    "user_agent = random.choice(user_agents_list)\n",
    "proxy = random.choice(proxies_list)\n",
    "\n",
    "# 設定 Chrome 瀏覽器選項\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(f\"user-agent={user_agent}\")\n",
    "chrome_options.add_argument(f\"--proxy-server=http://{proxy}\")\n",
    "\n",
    "# 設定 chromedriver 路徑\n",
    "service = Service(executable_path=\"./chromedriver.exe\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59c5b3-b15e-49db-a779-0ea042985aa9",
   "metadata": {},
   "source": [
    "#### 爬蟲主程式區"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a8ac84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已建立scraped_data資料夾\n",
      "開始進行手作\n",
      "第1頁爬取成功，並輸出csv完成\n",
      "第2頁爬取成功，並輸出csv完成\n",
      "手作 已爬取完畢，共爬取到第 2 頁，進行下一個分類\n",
      "開始進行音樂\n",
      "第1頁爬取成功，並輸出csv完成\n",
      "第2頁爬取成功，並輸出csv完成\n",
      "音樂 已爬取完畢，共爬取到第 2 頁，進行下一個分類\n",
      "開始進行攝影\n",
      "第1頁爬取成功，並輸出csv完成\n",
      "第2頁爬取成功，並輸出csv完成\n",
      "攝影第3頁發生錯誤並重試1次: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF78615AD02+56930]\n",
      "\t(No symbol) [0x00007FF7860CF602]\n",
      "\t(No symbol) [0x00007FF785F842E5]\n",
      "\t(No symbol) [0x00007FF785FC98ED]\n",
      "\t(No symbol) [0x00007FF785FC9A2C]\n",
      "\t(No symbol) [0x00007FF78600A967]\n",
      "\t(No symbol) [0x00007FF785FEBCDF]\n",
      "\t(No symbol) [0x00007FF7860081E2]\n",
      "\t(No symbol) [0x00007FF785FEBA43]\n",
      "\t(No symbol) [0x00007FF785FBD438]\n",
      "\t(No symbol) [0x00007FF785FBE4D1]\n",
      "\tGetHandleVerifier [0x00007FF7864D6F8D+3711213]\n",
      "\tGetHandleVerifier [0x00007FF7865304CD+4077101]\n",
      "\tGetHandleVerifier [0x00007FF78652865F+4044735]\n",
      "\tGetHandleVerifier [0x00007FF7861F9736+706710]\n",
      "\t(No symbol) [0x00007FF7860DB8DF]\n",
      "\t(No symbol) [0x00007FF7860D6AC4]\n",
      "\t(No symbol) [0x00007FF7860D6C1C]\n",
      "\t(No symbol) [0x00007FF7860C68D4]\n",
      "\tBaseThreadInitThunk [0x00007FFDFE277374+20]\n",
      "\tRtlUserThreadStart [0x00007FFDFF23CC91+33]\n",
      "\n",
      "第3頁爬取成功，並輸出csv完成\n",
      "第4頁爬取成功，並輸出csv完成\n",
      "攝影 已爬取完畢，共爬取到第 4 頁，進行下一個分類\n",
      "全部爬取完成,並關閉瀏覽器\n",
      "耗時358.2056610584259\n"
     ]
    }
   ],
   "source": [
    "# 建立資料夾\n",
    "folder_name = \"scraped_data\"\n",
    "\n",
    "# 如果資料夾不存在就新建立\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(\"已建立scraped_data資料夾\")\n",
    "\n",
    "\n",
    "# 開始計時並啟動webdriver\n",
    "start_time = time.time()\n",
    "driver = webdriver.Chrome(service=service,options=chrome_options)\n",
    "\n",
    "try:\n",
    "    # 瀏覽每個課程分類，逐頁爬取課程資料\n",
    "    for category_name, catagory_code in course_type.items():\n",
    "        page_number = 1 \n",
    "        print(f\"開始進行{category_name}\")\n",
    "\n",
    "        while True:\n",
    "            retries = 0 # 每一頁重試次數歸零\n",
    "            page_success = False\n",
    "            data = None\n",
    "\n",
    "            while retries < 3:\n",
    "            \n",
    "                try:\n",
    "                    # 進入目前頁面的 URL 並進入\n",
    "                    current_url = f\"https://hahow.in/group/{catagory_code}?filter=PUBLISHED&page={page_number}\"\n",
    "                    driver.get(current_url)\n",
    "\n",
    "                    # 等待頁面載入課程資料\n",
    "                    WebDriverWait(driver, 10).until(EC.presence_of_element_located(\n",
    "                        (By.XPATH, \"//*[@id='main-screen']/section[1]/div[5]/ul\")\n",
    "                    ))\n",
    "\n",
    "                    # 滾動到底部，確保內容完整載入\n",
    "                    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n",
    "\n",
    "                    # 取得目前頁面 HTML 並使用 BeautifulSoup 解析\n",
    "                    html = driver.page_source\n",
    "                    bs = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "                    # 擷取所有課程區塊\n",
    "                    data = bs.find_all(\"div\", \"sc-1awt4tz-1 gtWJhG\")\n",
    "\n",
    "                    # 若無課程資料，表示已到最後一頁，結束爬取\n",
    "                    if not data:\n",
    "                        print(f\"{category_name} 已爬取完畢，共爬取到第 {page_number - 1} 頁，進行下一個分類\")\n",
    "                        break\n",
    "\n",
    "                    else:\n",
    "                        # 呼叫自訂函數處理課程資料，轉為 DataFrame\n",
    "                        course_data = course_item(data)\n",
    "                        course_df = pd.DataFrame(course_data, columns=[\"課程名稱\", \"熱門標籤\", \"link\"])\n",
    "\n",
    "                        # 建立檔案路徑\n",
    "                        filename = os.path.join(folder_name,f\"hahow_{category_name}_page{page_number}.csv\")\n",
    "\n",
    "                        # 儲存 CSV 檔案\n",
    "                        course_df.to_csv(filename, encoding=\"utf-8-sig\", index=False)\n",
    "                        print(f\"第{page_number}頁爬取成功，並輸出csv完成\")\n",
    "\n",
    "                        # 前往下一頁\n",
    "                        page_success = True\n",
    "                        page_number += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    retries += 1\n",
    "                    print(f\"{category_name}第{page_number}頁發生錯誤並重試{retries}次:\", e)\n",
    "                    time.sleep(random.uniform(1, 1.5))\n",
    "            \n",
    "            if not page_success:\n",
    "               print(f\"⛔ {category_name}第{page_number}頁失敗 3 次，跳過此頁\")\n",
    "               page_number += 1\n",
    "                \n",
    "            if not data:\n",
    "                break\n",
    "                \n",
    "finally:\n",
    "    driver.quit()\n",
    "    print(\"全部爬取完成,並關閉瀏覽器\")\n",
    "\n",
    "# 計時結束\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"耗時{}\".format(execution_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f830a49-ad58-415f-b963-09e3c8c69125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
